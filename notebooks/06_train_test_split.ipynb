{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 Train Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Overview\n",
    "\n",
    "This notebook performs a **chronological 80/20 split** of the preprocessed dataset to prepare for time series modeling.\n",
    "\n",
    "**Key Steps:**\n",
    "\n",
    "* **Input:** Encoded, hourly-resampled dataset with engineered features\n",
    "* **Sorting:** Ensures time index is strictly ordered\n",
    "* **Split:** Reserves the final 20% of data as the forecasting holdout set\n",
    "* **Enrichment:** Applies feature engineering after the split to ensure no target leakage\n",
    "* **Output:** Saves `train.csv` and `forecast.csv` for baseline and ML model training\n",
    "\n",
    "> Purpose: Respect temporal order for causal integrity and prevent lookahead bias in forecasting models.\n",
    "\n",
    "### Thoughts, Tradeoffs & Considerations\n",
    "\n",
    "* **No shuffle allowed:** Time series models break if past and future are mixed. Chronological order is strictly preserved—this is **not optional** in forecasting tasks.\n",
    "* **Static 80/20 ratio:** Chose a fixed 80% train / 20% forecast split to simulate realistic deployment scenarios. Could be adjusted later depending on seasonality span or cross-validation design.\n",
    "* **Forecasting window:** With hourly data, 20% gives \\~2.4 months of holdout—enough to assess robustness across time patterns (e.g., day/night, weekday/weekend, weather shifts).\n",
    "* **Index integrity check:** Sorting the datetime index before the split was critical. Found minor time discontinuities earlier; now fully handled upstream.\n",
    "* **No temporal leakage:** Features like lagged values, weather, and time-based encodings must be computed **only from past data** during model training, this split enforces that discipline.\n",
    "* **Future tweak:** Could later introduce **rolling window** validation or walk-forward retraining, but for now a single static split is sufficient for baseline modeling and prototyping.\n",
    "\n",
    "> Main concern was **preserving causality and temporal realism**. Splitting randomly would give better metrics—but lie about deploy-time performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all rows and columns\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# widen the column width and overall display width\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df: pd.DataFrame = pd.read_csv(\"../data/interim/data_encoded.csv\", parse_dates=[\"time\"], index_col=\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define split point\n",
    "split_index = int(len(df) * 0.9)\n",
    "\n",
    "# Split\n",
    "train_df = df.iloc[:split_index]\n",
    "forecast_df = df.iloc[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lag_features(df: pd.DataFrame, lags: List[int], roll_windows: List[int]) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    for lag in lags:\n",
    "        df[f\"lag_{lag}\"] = df[\"use_house_overall\"].shift(lag)\n",
    "    for win in roll_windows:\n",
    "        df[f\"roll_mean_{win}\"] = df[\"use_house_overall\"].shift(1).rolling(window=win).mean()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = create_lag_features(train_df, lags=[1, 2, 3, 6, 12, 24, 48], roll_windows=[3, 6, 12, 24])\n",
    "forecast_df = create_lag_features(pd.concat([train_df.tail(12), forecast_df]), lags=[1, 2, 3], roll_windows=[3, 6, 12])\n",
    "forecast_df = forecast_df.loc[forecast_df.index.difference(train_df.index)]  # only keep new rows   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    # Appliance sum\n",
    "    appliance_cols = [\"winecellar\", \"barn\", \"fridge\", \"well\", \"dishwasher\", \"microwave\"]\n",
    "    df[\"appliance_sum\"] = df[appliance_cols].sum(axis=1)\n",
    "\n",
    "    # Furnace binary flag\n",
    "    df[\"furnace_on\"] = (df[\"furnace\"] > 0).astype(int)\n",
    "\n",
    "    # Net energy consumption\n",
    "    df[\"net_energy_lag_1\"] = df[\"lag_1\"] - df[\"generated_solar\"].shift(1)\n",
    "\n",
    "    # Hour block (e.g. 0–3 = 0, 4–7 = 1, ..., 20–23 = 5)\n",
    "    df[\"hour_block\"] = df[\"hour\"] // 4\n",
    "\n",
    "    # Weekend flag\n",
    "    df[\"is_weekend\"] = df[[\"wd_Saturday\", \"wd_Sunday\"]].sum(axis=1).clip(upper=1)\n",
    "\n",
    "    # Night flag (e.g. 0–6, 22–23)\n",
    "    df[\"is_night\"] = df[\"hour\"].isin([0, 1, 2, 3, 4, 5, 6, 22, 23]).astype(int)\n",
    "\n",
    "    # Winter flag (Dec, Jan, Feb)\n",
    "    df[\"is_winter\"] = df[\"month\"].isin([12, 1, 2]).astype(int)\n",
    "\n",
    "    # Day of year (for seasonal cycles)\n",
    "    df[\"dayofyear\"] = df.index.dayofyear\n",
    "    df[\"dayofyear_sin\"] = np.sin(2 * np.pi * df[\"dayofyear\"] / 365)\n",
    "    df[\"dayofyear_cos\"] = np.cos(2 * np.pi * df[\"dayofyear\"] / 365)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = enrich_features(train_df)\n",
    "forecast_df = enrich_features(forecast_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (7559, 61), Forecast shape: (840, 61)\n"
     ]
    }
   ],
   "source": [
    "train_df.to_csv(\"../data/interim/train.csv\", index=True)\n",
    "forecast_df.to_csv(\"../data/interim/forecast.csv\", index=True)\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}, Forecast shape: {forecast_df.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
